## 2018-09-05 更新
### 实例描述
- 程序版本： `2018-08-30 0044`
- 运行时间： `0829 2153 -- 0901 0930`

    > 共爬取 101870 个页面，其中：
    >
    > - 0k,1k,2k,3k 的有 2687 条
    > - 5k 的 9302 条
    > - 6k …… 50k 的若干
    > - 加载页面失败 1859 条

### 问题发现
- 之前没有考虑对话页存在分页的情况
- 所获取的 URL 链接的有对话保密页和医患关系关闭页
- 已经保存的页面有异常，引起对所有文件的质疑：
    - 出现文件大小为 `0k` 的文件，即文件空白
    - 出现文件大小为 `1234k` 的文件，即源码出现大量缺省，大多只剩下几个 HTML 标签
    - 出现文件大小为 `5k` 的文件，即基本为警示访问过于频繁的页面源码
    - 出现文件大小为 `17k` 的文件，即基本为医患关系关闭页
    - 出现文件大小为 `28k, 29k, 30k` 的文件，即基本为对话内容保密页面
    - 其余 `50k` 以下的文件大多可能是有缺省的，少数为完整源码
    - 由此质疑：当前程序无法保证任一个文件是完整的，即便该文件很大
- 2号中午异常终止，弹窗显示内存溢出
- 缺少一个断点续爬功能，精确到某日期页或者问诊对话页的重爬
- 分析：
    - 应该完整的页面类型为：问诊对话页及后续页，对话保密页，医患关系关闭页，访问频繁警示页
    - 异常类型为：
        - 链接未打开，转入 except
        - 以上每一种页面类型都可能出现缺省导致当前文件小于写入完整源码后文件大小
        - 某问诊对话有多页，非该对话最后一页判断是否有后续页的特征缺省，导致判断失败，未能获取后续页

### 原因探索
#### 可能的原因
1. 程序中 `webDriverWait` 能保证抓取完整页面，程序写入出了问题
1. 程序中 `browser.get()` 没有获取完整代码, `webDriverWait` 时间短了？
1. 程序中 `browser.get()` 获取了完整源码，经由以下命令执行后，导致源码部分丢失
    ```python
    elements = browser.find_element_by_xpath('//*')
    source_code = elements.get_attribute('outerHTML')
    ```

    > 网友声称用 `browser.page_source` 输出源码也存在缺省的情况

1. 之前一版程序运行过程中，`TIME_SLEEP` 值为 2 ，没有 `5k` 文件，这一版中应该是等待时间过短
1. 没有对内存的处理，程序运行时间过长以后造成溢出
1. 可能前述多种原因共同作用导致

#### 实验
1. 打开文件大小比较小的文件，与对应网页的源码进行比较
    - 结果发现存在缺省
1. 存储源码后，取大小，发现小于 `50k` 的就打印出来
    - 对比发现至少存在源码写入之前就不全的情况，暂无法确认是否有源码写入 TXT 文件的问题
    - 程序出现某种情况结束运行？本来想设置，没成功
    - 再次实验，可以同时再打印输出 `browser.page_source` ，对比看是否是由于获取源码方式导致缺省
    - 至于获取源码的方法，这里使用的 `selenium` 的方法，还有 `request`  的方法可以尝试

            urllib.request.urlopen(url).read()

1. 在当前版本程序，修改`TIME_SLEEP` 和 `TIME_WAIT` 分别为 2 和 20，结果如下：
    - 1212 —— 1240 期间：
        - 共保存文件 534 个
        - `0k` 文件为 4 个
        - `16k, 17k` 文件为 3 个
        - `28k` 文件为2个
    - 1212 —— 2130 期间：
        - 共保存文件 10045 个
        - `0k` 文件 189 个
        - `8k` 文件 1 个
        - `9k` 文件为 2 个
    - 09071212 —— 09081105 期间：
        - 共保存文件 26176 个
        - `1k` 的17个
        - 抓取失败的 1 个，其它的都正常

    从上述情况看，当前参数状态下，除了 `0k` 文件以外都正常

1. 在`TIME_SLEEP` 和 `TIME_WAIT` 分别为 2 和 20的基础上，将程序改为直接用 `browser.page_source` 输出源码

    > 第一次直接注释掉原获取源码的命令，执行 `file.write(browser.page_source)` ，log 输出全是获取失败
    >
    > 第二次令 `source_code = browser.page_source` ，然后输出 `source_code`，log 输出依然全是抓取失败
    >
    > 第三次经试验发现是`encoding`设置错误，在原输出源码的命令下，使用`gbk`编码是可以的，但是输出`browser.page_source` 编码方式只能用`gb18030`或者`utf-8`，所以和全局保持一致，输出正常(0911修改)

    使用`browser.page_source`截至目前，爬取的前2000条数据没有 `0k` 或 `5k` 的文件，所有文件都比较正常；从这里可以猜想获取源码的命令是不当的？在当前环境下，主要问题已经被解决，另处理翻页爬取的问题即可

1. 通过将 `browser.find_element_by_xpath('//li[@class="hh"]')` 中的 `hh` 改为 `kkk` 运行确认：这条语句可以有效判断该页是否有 `hh` 标签，如果没找到会进入 `except`

#### 其它未进行的实验
1. 写入问题判定

    设置判断条件，在出现大小为 `0k` 的文件的时候，就打印出 `source_code`，看下源码情况

1. 获取源码大小的方法尝试

    > `sys.getsizeof(source_code)`
    >
    > `len(source_code.encode('gbk'))`

1. 清除内存方法

    > `foo.del()`
    >
    > `gc.collect()`

1. 之前还考虑到是否将写入源码后的那个 `time_sleep()` 挪到紧邻写入源码的命令后

### 讨论
1. 通过实验，可以基本明确解决问题的可行方法
    - 之前没有发现同一个对话有多页，处理一下
    - `0k` 问题，将源码输出改为 `browser.page_source`
    - `1234k` 等文件缺省问题，`TIME_WAIT` 改成 20
    - `5k` 问题，将 `TIME_SLEEP` 改成 2
    - 医患关系关闭页和对话详情保密页照常存储
    - 内存溢出？参考：`foo.del()` 和 `gc.collect()` 显示清理用完的资源
    - 考虑增加断点续爬的功能：将已成功爬取的 url 的序号和它所在的日期页 url记入 TXT，然后程序运行之前读取该 TXT，不为空，则取出最后一个 url，作为参数输入，以此为起点开始爬

1. 完善程序的思路如下

    完善程序考虑的是尽可能让程序具备更大的容错能力和实现目标的能力，这里主要是对 `0k` 等系列问题积极主动解决，而非像上述采用调参来规避

    - 同上的系列调参和修改输出源码的方法，可以出于效率的考虑，不用将参数调至规避所有异常，异常情况结合下述部分解决
    - 异常解决步骤：
        - A.传入初始的 url，抓取源码后判断是否为保密页或关闭医患关系页，不保存并统计 url
        - B.获取网页源码后，保存网页到 TXT 文件，获取文件大小（直接取源码字节数？）：

            > 获取源码大小的方法：
            >
            > - `os.path.getsize()`
            > - `len(source_code.encode('gbk'))`
            > - `sys.getsizeof(source_code)`
            >
            > 后两种未进行尝试

            - 文件小于50k，重新爬取，循环三次 + time.sleep(5)

                > 这个小于多少 k 的值不好确定，考虑到50以下都不是很多，可以这样

            - 如果依然小于50k则舍去该 url 并保存 url

                > 就担心 50k 以下也有正常页面

            - 如果大于50k：

                > 这里考虑缺省的问题，最好在页面中寻找两个标签进行判断

                - 在文件中寻找对话结束的标签或下一页的标签，判断有无下一页
                    - 有则读取页数，构造 url 爬取，爬取调用函数完成
                    - 无则跳过

    为了确保数据的完整性，还可以同时多线程爬取同一个时间段的数据，然后存入不同的文件夹中，取文件大小进行对比，将最大的一个放入新建的文件夹中，最后新建的文件夹中就是最优的数据集。这一个操作既可以结合上述使用，也可以单独使用。

### 方案

目前时间有限，先按照最基本的做法进行修改

- 调参与修改获取源码方法

        TIME_WAIT = 30
        TIME_SLEEP = 2

    ```python
    source_code = browser.page_source
    file_name = pre_file_name + detail_page_url[28:]
    with open(file_path + file_name + '.txt', 'w', encoding='gbk') as file:
        file.write(source_code)
    ```

- 实现爬取当前对话后续页的功能
    - 解析网页判定含有 `mt50` 后，解析网页 url，set 去重
    - 解析网页判定含有 `mt50` 后，构造 url 爬取直到该页没有对话流标签，这样网页访问量会增大
    - [x] 使用正则表达式获取总页数，然后针对性构造 url，这里选用这种方法
        - 取消 try 嵌套
        - 爬取后续页的部分的代码对齐
        - 源码输出编码方式修改为`gb18030`
        - 版本：2018-09-11 2052
- 实现断点续传功能
    - 实现某条语句在 for循环中只执行一次
        - 使用状态量，i 初值为1，循环中做减法，判断 i大于等于1时执行某语句
        - 使用状态量，初始为 true，循环中初次执行某语句后改为 false
        - [x] 在函数中修改全局变量，python把0，[ ], none看成 false，

            > python 中，如果全局变量为普通变量，则在函数中变量前加global
            >
            > 如果全局变量为 list 对象，则直接修改即可，限 `a[0] = 2`等，如 `a = [3, 4]`则不可，[详见](https://www.cnblogs.com/yanfengt/p/6305542.html)

        - 修饰符，[循环中函数的有效方式只执行一次](https://codeday.me/bug/20170901/65228.html)

    - 用一个文件存储状态，只保留最新的即可
    - [ ] 精确到续爬某问诊记录的某后续页也可以用同样的方法实现，用'a'，需要保存该问诊记录总页数和当前断点页
    - 解析时用考虑用set 去重
    - timestamp：2018-09-12 0245
    - [ ] 可以利用全局变量和 regex 构造函数，程序开始运行时执行该函数`crawl_from_breakpoint`，读取文本中值改变全局变量，实现无需调参自动续爬
        - 如果每次续爬重新生成一个文件夹，那么注意状态文本的路径变化
<meta http-equiv = "refresh" content = "1.0">





